{
    "base_model_path": "",
    "save_model_path": "",
    "generation_config": {
        "bos_token_id": 128000,
        "eos_token_id": 128001,
        "do_sample": true,
        "temperature": 0.9,
        "max_length": 1024,
        "top_p": 0.8,
        "repetition_penalty": 1.1,
        "transformers_version": "4.40.0.dev0"
    },
    "lora_config": {
        "r": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "bias": "none",
        "task_type": "CAUSAL_LM",
        "init_lora_weights": "gaussian",
        "target_modules": ["up_proj", "gate_proj", "down_proj"]
    },
    "training_args": {
        "per_device_train_batch_size": 2,
        "per_device_eval_batch_size": 2,
        "num_train_epochs": 1,
        "eval_strategy": "epoch",
        "logging_strategy": "steps",
        "logging_steps": 100,
        "save_strategy": "epoch",
        "learning_rate": 2e-5,
        "gradient_accumulation_steps": 4,
        "report_to": "none"
    },
    "testcases": [
        {
            "lora_name": "alpaca_train_partial",
            "dataset_type": "json",
            "dataset_files": {
                "train": "train.jsonl"
            },
            "output_dir": "./lora_finetuned_alpaca_partial"
        }
    ]
}